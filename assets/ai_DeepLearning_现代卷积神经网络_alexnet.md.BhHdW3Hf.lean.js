import{_ as h,c as t,o as l,ag as n,j as i,a}from"./chunks/framework.C2Gjomh7.js";const e="/MyBlog/assets/filters.DLjnf6TJ.png",k="/MyBlog/assets/alexnet.CFllqKeE.svg",P=JSON.parse('{"title":"深度卷积神经网络（AlexNet）","description":"","frontmatter":{},"headers":[],"relativePath":"ai/DeepLearning/现代卷积神经网络/alexnet.md","filePath":"ai/DeepLearning/现代卷积神经网络/alexnet.md","lastUpdated":1756829839000}'),p={name:"ai/DeepLearning/现代卷积神经网络/alexnet.md"},r={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},d={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"0"},xmlns:"http://www.w3.org/2000/svg",width:"5.028ex",height:"1.532ex",role:"img",focusable:"false",viewBox:"0 -677 2222.4 677","aria-hidden":"true"},E={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},g={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.05ex"},xmlns:"http://www.w3.org/2000/svg",width:"10.936ex",height:"1.557ex",role:"img",focusable:"false",viewBox:"0 -666 4833.6 688","aria-hidden":"true"},o={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},y={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.566ex"},xmlns:"http://www.w3.org/2000/svg",width:"12.57ex",height:"2.262ex",role:"img",focusable:"false",viewBox:"0 -750 5556 1000","aria-hidden":"true"},T={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},F={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"0"},xmlns:"http://www.w3.org/2000/svg",width:"7.291ex",height:"1.507ex",role:"img",focusable:"false",viewBox:"0 -666 3222.4 666","aria-hidden":"true"},Q={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},m={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.05ex"},xmlns:"http://www.w3.org/2000/svg",width:"5.028ex",height:"1.557ex",role:"img",focusable:"false",viewBox:"0 -666 2222.4 688","aria-hidden":"true"},C={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},A={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.05ex"},xmlns:"http://www.w3.org/2000/svg",width:"5.028ex",height:"1.554ex",role:"img",focusable:"false",viewBox:"0 -665 2222.4 687","aria-hidden":"true"},c={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},u={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.05ex"},xmlns:"http://www.w3.org/2000/svg",width:"5.028ex",height:"1.554ex",role:"img",focusable:"false",viewBox:"0 -665 2222.4 687","aria-hidden":"true"},x={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},D={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.05ex"},xmlns:"http://www.w3.org/2000/svg",width:"7.291ex",height:"1.557ex",role:"img",focusable:"false",viewBox:"0 -666 3222.4 688","aria-hidden":"true"},B={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},w={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"0"},xmlns:"http://www.w3.org/2000/svg",width:"9.553ex",height:"1.532ex",role:"img",focusable:"false",viewBox:"0 -677 4222.4 677","aria-hidden":"true"},L={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},_={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.05ex"},xmlns:"http://www.w3.org/2000/svg",width:"7.291ex",height:"1.557ex",role:"img",focusable:"false",viewBox:"0 -666 3222.4 688","aria-hidden":"true"};function f(V,s,v,b,H,M){return l(),t("div",null,[s[54]||(s[54]=n("",17)),i("p",null,[s[2]||(s[2]=a("深度学习对计算资源要求很高，训练可能需要数百个迭代轮数，每次迭代都需要通过代价高昂的许多线性代数层传递数据。这也是为什么在20世纪90年代至21世纪初，优化凸目标的简单算法是研究人员的首选。然而，用GPU训练神经网络改变了这一格局。",-1)),s[3]||(s[3]=i("em",null,"图形处理器",-1)),s[4]||(s[4]=a("（Graphics Processing Unit，GPU）早年用来加速图形处理，使电脑游戏玩家受益。GPU可优化高吞吐量的",-1)),i("mjx-container",r,[(l(),t("svg",d,[...s[0]||(s[0]=[n("",1)])])),s[1]||(s[1]=i("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[i("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[i("mn",null,"4"),i("mo",null,"×"),i("mn",null,"4")])],-1))]),s[5]||(s[5]=a("矩阵和向量乘法，从而服务于基本的图形任务。幸运的是，这些数学运算与卷积层的计算惊人地相似。由此，英伟达（NVIDIA）和ATI已经开始为通用计算操作优化gpu，甚至把它们作为",-1)),s[6]||(s[6]=i("em",null,"通用GPU",-1)),s[7]||(s[7]=a("（general-purpose GPUs，GPGPU）来销售。",-1))]),s[55]||(s[55]=i("p",null,"那么GPU比CPU强在哪里呢？",-1)),s[56]||(s[56]=i("p",null,[a("首先，我们深度理解一下中央处理器（Central Processing Unit，CPU）的"),i("em",null,"核心"),a("。 CPU的每个核心都拥有高时钟频率的运行能力，和高达数MB的三级缓存（L3Cache）。 它们非常适合执行各种指令，具有分支预测器、深层流水线和其他使CPU能够运行各种程序的功能。 然而，这种明显的优势也是它的致命弱点：通用核心的制造成本非常高。 它们需要大量的芯片面积、复杂的支持结构（内存接口、内核之间的缓存逻辑、高速互连等等），而且它们在任何单个任务上的性能都相对较差。 现代笔记本电脑最多有4核，即使是高端服务器也很少超过64核，因为它们的性价比不高。")],-1)),i("p",null,[s[12]||(s[12]=a("相比于CPU，GPU由",-1)),i("mjx-container",E,[(l(),t("svg",g,[...s[8]||(s[8]=[n("",1)])])),s[9]||(s[9]=i("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[i("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[i("mn",null,"100"),i("mo",null,"∼"),i("mn",null,"1000")])],-1))]),s[13]||(s[13]=a("个小的处理单元组成（NVIDIA、ATI、ARM和其他芯片供应商之间的细节稍有不同），通常被分成更大的组（NVIDIA称之为warps）。 虽然每个GPU核心都相对较弱，有时甚至以低于1GHz的时钟频率运行，但庞大的核心数量使GPU比CPU快几个数量级。 例如，NVIDIA最近一代的Ampere GPU架构为每个芯片提供了高达312 TFlops的浮点性能，而CPU的浮点性能到目前为止还没有超过1 TFlops。 之所以有如此大的差距，原因其实很简单：首先，功耗往往会随时钟频率呈二次方增长。 对于一个CPU核心，假设它的运行速度比GPU快4倍，但可以使用16个GPU核代替，那么GPU的综合性能就是CPU的",-1)),i("mjx-container",o,[(l(),t("svg",y,[...s[10]||(s[10]=[n("",1)])])),s[11]||(s[11]=i("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[i("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[i("mn",null,"16"),i("mo",null,"×"),i("mn",null,"1"),i("mrow",{"data-mjx-texclass":"ORD"},[i("mo",null,"/")]),i("mn",null,"4"),i("mo",null,"="),i("mn",null,"4")])],-1))]),s[14]||(s[14]=a("倍。 其次，GPU内核要简单得多，这使得它们更节能。 此外，深度学习中的许多操作需要相对较高的内存带宽，而GPU拥有10倍于CPU的带宽。",-1))]),s[57]||(s[57]=n("",9)),i("p",null,[s[23]||(s[23]=a("在AlexNet的第一层，卷积窗口的形状是",-1)),i("mjx-container",T,[(l(),t("svg",F,[...s[15]||(s[15]=[n("",1)])])),s[16]||(s[16]=i("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[i("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[i("mn",null,"11"),i("mo",null,"×"),i("mn",null,"11")])],-1))]),s[24]||(s[24]=a("。 由于ImageNet中大多数图像的宽和高比MNIST图像的多10倍以上，因此，需要一个更大的卷积窗口来捕获目标。 第二层中的卷积窗口形状被缩减为",-1)),i("mjx-container",Q,[(l(),t("svg",m,[...s[17]||(s[17]=[n("",1)])])),s[18]||(s[18]=i("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[i("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[i("mn",null,"5"),i("mo",null,"×"),i("mn",null,"5")])],-1))]),s[25]||(s[25]=a("，然后是",-1)),i("mjx-container",C,[(l(),t("svg",A,[...s[19]||(s[19]=[n("",1)])])),s[20]||(s[20]=i("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[i("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[i("mn",null,"3"),i("mo",null,"×"),i("mn",null,"3")])],-1))]),s[26]||(s[26]=a("。 此外，在第一层、第二层和第五层卷积层之后，加入窗口形状为",-1)),i("mjx-container",c,[(l(),t("svg",u,[...s[21]||(s[21]=[n("",1)])])),s[22]||(s[22]=i("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[i("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[i("mn",null,"3"),i("mo",null,"×"),i("mn",null,"3")])],-1))]),s[27]||(s[27]=a("、步幅为2的最大汇聚层。 而且，AlexNet的卷积通道数目是LeNet的10倍。",-1))]),s[58]||(s[58]=n("",15)),i("p",null,[s[33]||(s[33]=a("尽管原文中AlexNet是在ImageNet上进行训练的，但本书在这里使用的是Fashion-MNIST数据集。因为即使在现代GPU上，训练ImageNet模型，同时使其收敛可能需要数小时或数天的时间。 将AlexNet直接应用于Fashion-MNIST的一个问题是，[",-1)),s[34]||(s[34]=i("strong",null,"Fashion-MNIST图像的分辨率",-1)),s[35]||(s[35]=a("]（",-1)),i("mjx-container",x,[(l(),t("svg",D,[...s[28]||(s[28]=[n("",1)])])),s[29]||(s[29]=i("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[i("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[i("mn",null,"28"),i("mo",null,"×"),i("mn",null,"28")])],-1))]),s[36]||(s[36]=a("像素）(",-1)),s[37]||(s[37]=i("strong",null,"低于ImageNet图像。",-1)),s[38]||(s[38]=a(") 为了解决这个问题，(",-1)),i("strong",null,[s[32]||(s[32]=a("我们将它们增加到",-1)),i("mjx-container",B,[(l(),t("svg",w,[...s[30]||(s[30]=[n("",1)])])),s[31]||(s[31]=i("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[i("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[i("mn",null,"224"),i("mo",null,"×"),i("mn",null,"224")])],-1))])]),s[39]||(s[39]=a(")（通常来讲这不是一个明智的做法，但在这里这样做是为了有效使用AlexNet架构）。 这里需要使用",-1)),s[40]||(s[40]=i("code",null,"d2l.load_data_fashion_mnist",-1)),s[41]||(s[41]=a("函数中的",-1)),s[42]||(s[42]=i("code",null,"resize",-1)),s[43]||(s[43]=a("参数执行此调整。",-1))]),s[59]||(s[59]=n("",7)),i("ol",null,[s[50]||(s[50]=i("li",null,"试着增加迭代轮数。对比LeNet的结果有什么不同？为什么？",-1)),i("li",null,[s[49]||(s[49]=a("AlexNet对Fashion-MNIST数据集来说可能太复杂了。 ",-1)),i("ol",null,[s[48]||(s[48]=i("li",null,"尝试简化模型以加快训练速度，同时确保准确性不会显著下降。",-1)),i("li",null,[s[46]||(s[46]=a("设计一个更好的模型，可以直接在",-1)),i("mjx-container",L,[(l(),t("svg",_,[...s[44]||(s[44]=[n("",1)])])),s[45]||(s[45]=i("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[i("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[i("mn",null,"28"),i("mo",null,"×"),i("mn",null,"28")])],-1))]),s[47]||(s[47]=a("图像上工作。",-1))])])]),s[51]||(s[51]=i("li",null,"修改批量大小，并观察模型精度和GPU显存变化。",-1)),s[52]||(s[52]=i("li",null,[a("分析了AlexNet的计算性能。 "),i("ol",null,[i("li",null,"在AlexNet中主要是哪部分占用显存？"),i("li",null,"在AlexNet中主要是哪部分需要更多的计算？"),i("li",null,"计算结果时显存带宽如何？")])],-1)),s[53]||(s[53]=i("li",null,"将dropout和ReLU应用于LeNet-5，效果有提升吗？再试试预处理会怎么样？",-1))]),s[60]||(s[60]=n("",4))])}const S=h(p,[["render",f]]);export{P as __pageData,S as default};
